{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e75a2",
   "metadata": {
    "id": "c22e75a2"
   },
   "outputs": [],
   "source": [
    "# data link = 'https://www.kaggle.com/datasets/nphantawee/pump-sensor-data'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1726049",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c1726049",
    "outputId": "a2187452-0dc3-4c83-b735-efccf17dbcc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6206a6",
   "metadata": {
    "id": "df6206a6"
   },
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "random.seed(777)\n",
    "torch.manual_seed(777)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0148c27",
   "metadata": {
    "id": "e0148c27"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data_url = \"/content/drive/MyDrive/ECE 453 project/sensor.csv\"\n",
    "df = pd.read_csv(train_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TevshdL49osu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TevshdL49osu",
    "outputId": "6415fad5-96b0-4885-cb9e-d44695d69465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da43e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "01da43e4",
    "outputId": "cb97fa27-3a49-4444-9f11-1957237b80e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9a340083-ada2-4248-922d-d5250777eded\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor_00</th>\n",
       "      <th>sensor_01</th>\n",
       "      <th>sensor_02</th>\n",
       "      <th>sensor_03</th>\n",
       "      <th>sensor_04</th>\n",
       "      <th>sensor_05</th>\n",
       "      <th>sensor_06</th>\n",
       "      <th>sensor_07</th>\n",
       "      <th>sensor_08</th>\n",
       "      <th>sensor_09</th>\n",
       "      <th>...</th>\n",
       "      <th>sensor_43</th>\n",
       "      <th>sensor_44</th>\n",
       "      <th>sensor_45</th>\n",
       "      <th>sensor_46</th>\n",
       "      <th>sensor_47</th>\n",
       "      <th>sensor_48</th>\n",
       "      <th>sensor_49</th>\n",
       "      <th>sensor_50</th>\n",
       "      <th>sensor_51</th>\n",
       "      <th>machine_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.211800</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.375000</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>15.05353</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.465394</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.211800</td>\n",
       "      <td>46.310760</td>\n",
       "      <td>634.375000</td>\n",
       "      <td>76.45975</td>\n",
       "      <td>13.41146</td>\n",
       "      <td>16.13136</td>\n",
       "      <td>15.56713</td>\n",
       "      <td>15.05353</td>\n",
       "      <td>...</td>\n",
       "      <td>41.92708</td>\n",
       "      <td>39.641200</td>\n",
       "      <td>65.68287</td>\n",
       "      <td>50.92593</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>157.9861</td>\n",
       "      <td>67.70834</td>\n",
       "      <td>243.0556</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.444734</td>\n",
       "      <td>47.35243</td>\n",
       "      <td>53.211800</td>\n",
       "      <td>46.397570</td>\n",
       "      <td>638.888900</td>\n",
       "      <td>73.54598</td>\n",
       "      <td>13.32465</td>\n",
       "      <td>16.03733</td>\n",
       "      <td>15.61777</td>\n",
       "      <td>15.01013</td>\n",
       "      <td>...</td>\n",
       "      <td>41.66666</td>\n",
       "      <td>39.351852</td>\n",
       "      <td>65.39352</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194443</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>67.12963</td>\n",
       "      <td>241.3194</td>\n",
       "      <td>203.7037</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.460474</td>\n",
       "      <td>47.09201</td>\n",
       "      <td>53.168400</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>628.125000</td>\n",
       "      <td>76.98898</td>\n",
       "      <td>13.31742</td>\n",
       "      <td>16.24711</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>15.08247</td>\n",
       "      <td>...</td>\n",
       "      <td>40.88541</td>\n",
       "      <td>39.062500</td>\n",
       "      <td>64.81481</td>\n",
       "      <td>51.21528</td>\n",
       "      <td>38.194440</td>\n",
       "      <td>155.9606</td>\n",
       "      <td>66.84028</td>\n",
       "      <td>240.4514</td>\n",
       "      <td>203.1250</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.445718</td>\n",
       "      <td>47.13541</td>\n",
       "      <td>53.211800</td>\n",
       "      <td>46.397568</td>\n",
       "      <td>636.458300</td>\n",
       "      <td>76.58897</td>\n",
       "      <td>13.35359</td>\n",
       "      <td>16.21094</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>15.08247</td>\n",
       "      <td>...</td>\n",
       "      <td>41.40625</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>65.10416</td>\n",
       "      <td>51.79398</td>\n",
       "      <td>38.773150</td>\n",
       "      <td>158.2755</td>\n",
       "      <td>66.55093</td>\n",
       "      <td>242.1875</td>\n",
       "      <td>201.3889</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220315</th>\n",
       "      <td>2.407350</td>\n",
       "      <td>47.69965</td>\n",
       "      <td>50.520830</td>\n",
       "      <td>43.142361</td>\n",
       "      <td>634.722229</td>\n",
       "      <td>64.59095</td>\n",
       "      <td>15.11863</td>\n",
       "      <td>16.65220</td>\n",
       "      <td>15.65393</td>\n",
       "      <td>15.16204</td>\n",
       "      <td>...</td>\n",
       "      <td>38.28125</td>\n",
       "      <td>68.287030</td>\n",
       "      <td>52.37268</td>\n",
       "      <td>48.32176</td>\n",
       "      <td>41.087960</td>\n",
       "      <td>212.3843</td>\n",
       "      <td>153.64580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.1921</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220316</th>\n",
       "      <td>2.400463</td>\n",
       "      <td>47.69965</td>\n",
       "      <td>50.564240</td>\n",
       "      <td>43.142361</td>\n",
       "      <td>630.902771</td>\n",
       "      <td>65.83363</td>\n",
       "      <td>15.15480</td>\n",
       "      <td>16.70284</td>\n",
       "      <td>15.65393</td>\n",
       "      <td>15.11863</td>\n",
       "      <td>...</td>\n",
       "      <td>38.28125</td>\n",
       "      <td>66.840280</td>\n",
       "      <td>50.63657</td>\n",
       "      <td>48.03241</td>\n",
       "      <td>40.798610</td>\n",
       "      <td>213.8310</td>\n",
       "      <td>156.25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231.1921</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220317</th>\n",
       "      <td>2.396528</td>\n",
       "      <td>47.69965</td>\n",
       "      <td>50.520830</td>\n",
       "      <td>43.142361</td>\n",
       "      <td>625.925903</td>\n",
       "      <td>67.29445</td>\n",
       "      <td>15.08970</td>\n",
       "      <td>16.70284</td>\n",
       "      <td>15.69734</td>\n",
       "      <td>15.11863</td>\n",
       "      <td>...</td>\n",
       "      <td>39.06250</td>\n",
       "      <td>65.393520</td>\n",
       "      <td>48.90046</td>\n",
       "      <td>48.03241</td>\n",
       "      <td>40.798610</td>\n",
       "      <td>217.3032</td>\n",
       "      <td>155.38190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232.0602</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220318</th>\n",
       "      <td>2.406366</td>\n",
       "      <td>47.69965</td>\n",
       "      <td>50.520832</td>\n",
       "      <td>43.142361</td>\n",
       "      <td>635.648100</td>\n",
       "      <td>65.09175</td>\n",
       "      <td>15.11863</td>\n",
       "      <td>16.56539</td>\n",
       "      <td>15.74074</td>\n",
       "      <td>15.11863</td>\n",
       "      <td>...</td>\n",
       "      <td>40.62500</td>\n",
       "      <td>64.236110</td>\n",
       "      <td>47.74306</td>\n",
       "      <td>48.32176</td>\n",
       "      <td>40.509258</td>\n",
       "      <td>222.5116</td>\n",
       "      <td>153.93520</td>\n",
       "      <td>NaN</td>\n",
       "      <td>234.0856</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220319</th>\n",
       "      <td>2.396528</td>\n",
       "      <td>47.69965</td>\n",
       "      <td>50.520832</td>\n",
       "      <td>43.142361</td>\n",
       "      <td>639.814800</td>\n",
       "      <td>65.45634</td>\n",
       "      <td>15.11863</td>\n",
       "      <td>16.65220</td>\n",
       "      <td>15.65393</td>\n",
       "      <td>15.01013</td>\n",
       "      <td>...</td>\n",
       "      <td>41.40625</td>\n",
       "      <td>62.789350</td>\n",
       "      <td>46.29630</td>\n",
       "      <td>48.90046</td>\n",
       "      <td>40.219910</td>\n",
       "      <td>227.4306</td>\n",
       "      <td>150.46300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>234.0856</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220320 rows × 53 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a340083-ada2-4248-922d-d5250777eded')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9a340083-ada2-4248-922d-d5250777eded button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9a340083-ada2-4248-922d-d5250777eded');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "        sensor_00  sensor_01  sensor_02  sensor_03   sensor_04  sensor_05  \\\n",
       "0        2.465394   47.09201  53.211800  46.310760  634.375000   76.45975   \n",
       "1        2.465394   47.09201  53.211800  46.310760  634.375000   76.45975   \n",
       "2        2.444734   47.35243  53.211800  46.397570  638.888900   73.54598   \n",
       "3        2.460474   47.09201  53.168400  46.397568  628.125000   76.98898   \n",
       "4        2.445718   47.13541  53.211800  46.397568  636.458300   76.58897   \n",
       "...           ...        ...        ...        ...         ...        ...   \n",
       "220315   2.407350   47.69965  50.520830  43.142361  634.722229   64.59095   \n",
       "220316   2.400463   47.69965  50.564240  43.142361  630.902771   65.83363   \n",
       "220317   2.396528   47.69965  50.520830  43.142361  625.925903   67.29445   \n",
       "220318   2.406366   47.69965  50.520832  43.142361  635.648100   65.09175   \n",
       "220319   2.396528   47.69965  50.520832  43.142361  639.814800   65.45634   \n",
       "\n",
       "        sensor_06  sensor_07  sensor_08  sensor_09  ...  sensor_43  sensor_44  \\\n",
       "0        13.41146   16.13136   15.56713   15.05353  ...   41.92708  39.641200   \n",
       "1        13.41146   16.13136   15.56713   15.05353  ...   41.92708  39.641200   \n",
       "2        13.32465   16.03733   15.61777   15.01013  ...   41.66666  39.351852   \n",
       "3        13.31742   16.24711   15.69734   15.08247  ...   40.88541  39.062500   \n",
       "4        13.35359   16.21094   15.69734   15.08247  ...   41.40625  38.773150   \n",
       "...           ...        ...        ...        ...  ...        ...        ...   \n",
       "220315   15.11863   16.65220   15.65393   15.16204  ...   38.28125  68.287030   \n",
       "220316   15.15480   16.70284   15.65393   15.11863  ...   38.28125  66.840280   \n",
       "220317   15.08970   16.70284   15.69734   15.11863  ...   39.06250  65.393520   \n",
       "220318   15.11863   16.56539   15.74074   15.11863  ...   40.62500  64.236110   \n",
       "220319   15.11863   16.65220   15.65393   15.01013  ...   41.40625  62.789350   \n",
       "\n",
       "        sensor_45  sensor_46  sensor_47  sensor_48  sensor_49  sensor_50  \\\n",
       "0        65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
       "1        65.68287   50.92593  38.194440   157.9861   67.70834   243.0556   \n",
       "2        65.39352   51.21528  38.194443   155.9606   67.12963   241.3194   \n",
       "3        64.81481   51.21528  38.194440   155.9606   66.84028   240.4514   \n",
       "4        65.10416   51.79398  38.773150   158.2755   66.55093   242.1875   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "220315   52.37268   48.32176  41.087960   212.3843  153.64580        NaN   \n",
       "220316   50.63657   48.03241  40.798610   213.8310  156.25000        NaN   \n",
       "220317   48.90046   48.03241  40.798610   217.3032  155.38190        NaN   \n",
       "220318   47.74306   48.32176  40.509258   222.5116  153.93520        NaN   \n",
       "220319   46.29630   48.90046  40.219910   227.4306  150.46300        NaN   \n",
       "\n",
       "        sensor_51  machine_status  \n",
       "0        201.3889          NORMAL  \n",
       "1        201.3889          NORMAL  \n",
       "2        203.7037          NORMAL  \n",
       "3        203.1250          NORMAL  \n",
       "4        201.3889          NORMAL  \n",
       "...           ...             ...  \n",
       "220315   231.1921          NORMAL  \n",
       "220316   231.1921          NORMAL  \n",
       "220317   232.0602          NORMAL  \n",
       "220318   234.0856          NORMAL  \n",
       "220319   234.0856          NORMAL  \n",
       "\n",
       "[220320 rows x 53 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unecessary column\n",
    "df = df.iloc[: , 1:] \n",
    "del df['timestamp']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c52c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 700
    },
    "id": "4d5c52c6",
    "outputId": "1c23ae2e-4307-4ccc-c689-1bfa2a3eb39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "5         1\n",
       "         ..\n",
       "220315    1\n",
       "220316    1\n",
       "220317    1\n",
       "220318    1\n",
       "220319    1\n",
       "Name: machine_status, Length: 217444, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "# change data type ( for sensor to numeric)\n",
    "for var_index in [item for item in df.columns if 'sensor_' in item]:\n",
    "  df[var_index] = pd.to_numeric(df[var_index], errors = 'coerce')\n",
    "\n",
    "# find Null\n",
    "(df.isnull().sum()/len(df)).plot.bar(figsize=(18, 8), colormap='Paired')\n",
    "\n",
    "# since sensor 15 is all null, sensor 50 is 50% null, delete\n",
    "# other sensors with null, use previous time's data for null\n",
    "\n",
    "# delete duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# delete sensor 15, 50\n",
    "del df['sensor_15']\n",
    "del df['sensor_50']\n",
    "\n",
    "# fill null for previous data\n",
    "df = df.fillna(method = 'ffill')\n",
    "\n",
    "# check if there is still nan (False = No, True = Yes)\n",
    "\n",
    "check_for_nan = df.isnull().values.any()\n",
    "print (check_for_nan)\n",
    "\n",
    "# Change string to the number (normal = 1, others = 0)\n",
    "Status = { \"NORMAL\": 1, \"BROKEN\": 0, \"RECOVERING\":0}\n",
    "state = df[\"machine_status\"].replace(Status)\n",
    "df['machine_status']=state\n",
    "\n",
    "df['machine_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c432b8",
   "metadata": {
    "id": "56c432b8"
   },
   "outputs": [],
   "source": [
    "# split x, y\n",
    "input_x = df.drop('machine_status', axis=1).values\n",
    "input_y = df['machine_status'].values\n",
    "\n",
    "n_features = input_x.shape[1]\n",
    "\n",
    "# LSTM needs 3dim data shape, change it to 3 dim using timestep\n",
    "\n",
    "def temporalize(X, y, timesteps):\n",
    "\toutput_X = []\n",
    "\toutput_y = []\n",
    "\tfor i in range(len(X) - timesteps - 1):\n",
    "\t\tt = []\n",
    "\t\tfor j in range(1, timesteps + 1):\n",
    "\t\t\t# Gather the past records upto the lookback period\n",
    "\t\t\tt.append(X[[(i + j + 1)], :])\n",
    "\t\toutput_X.append(t)\n",
    "\t\toutput_y.append(y[i + timesteps + 1])\n",
    "\treturn np.squeeze(np.array(output_X)), np.array(output_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b378a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "164b378a",
    "outputId": "fc5ab511-274e-45b7-a3ad-c59f0a654c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217438, 5, 50)\n",
      "length of x_train :  139160\n",
      "length of x_valid :  34790\n",
      "length of x_test :  43488\n",
      "(9113, 5, 50)\n"
     ]
    }
   ],
   "source": [
    "# define timestep as 5 min\n",
    "timesteps = 5\n",
    "# Temporalize\n",
    "x, y = temporalize(input_x, input_y, timesteps)\n",
    "print(x.shape) \n",
    "\n",
    "# Split into train, valid, and test \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2)\n",
    "\n",
    "print('length of x_train : ',len(x_train))  \n",
    "print('length of x_valid : ',len(x_valid))  \n",
    "print('length of x_test : ',len(x_test))   \n",
    "\n",
    "# For training the autoencoder, split 0 / 1\n",
    "# use only normal data for training\n",
    "x_train_y0 = x_train[y_train == 0]\n",
    "x_train_y1 = x_train[y_train == 1]\n",
    "\n",
    "x_valid_y0 = x_valid[y_valid == 0]\n",
    "x_valid_y1 = x_valid[y_valid == 1]\n",
    "print(x_train_y0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574a984f",
   "metadata": {
    "id": "574a984f"
   },
   "outputs": [],
   "source": [
    "# Use flatten since scaler needs 2D\n",
    "def flatten(X):\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "\n",
    "def scale(X, scaler):\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
    "        \n",
    "    return X\n",
    "\n",
    "# Standardization for different data characteristics\n",
    "scaler = StandardScaler().fit(flatten(x_train_y1))\n",
    "\n",
    "x_train_y1_scaled = scale(x_train_y1, scaler)\n",
    "x_valid_scaled = scale(x_valid, scaler)\n",
    "x_valid_y1_scaled = scale(x_valid_y1, scaler)\n",
    "x_test_scaled = scale(x_test, scaler)\n",
    "\n",
    "# define parameters\n",
    "epochs = 200\n",
    "batch = 128\n",
    "lr = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c1174",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a53c1174",
    "outputId": "8a8697fc-609c-4daf-b3da-b172356349a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 5, 32)             10624     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 16)                3136      \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 5, 16)            0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 5, 16)             2112      \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 5, 32)             6272      \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 5, 50)            1650      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,794\n",
      "Trainable params: 23,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1016/1016 [==============================] - 22s 17ms/step - loss: 0.3716 - val_loss: 0.2221\n",
      "Epoch 2/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1956 - val_loss: 0.1700\n",
      "Epoch 3/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.1606 - val_loss: 0.1452\n",
      "Epoch 4/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1393 - val_loss: 0.1272\n",
      "Epoch 5/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1260 - val_loss: 0.1189\n",
      "Epoch 6/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1169 - val_loss: 0.1111\n",
      "Epoch 7/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1112 - val_loss: 0.1064\n",
      "Epoch 8/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1076 - val_loss: 0.1039\n",
      "Epoch 9/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.1035 - val_loss: 0.0999\n",
      "Epoch 10/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.1008 - val_loss: 0.0961\n",
      "Epoch 11/200\n",
      "1016/1016 [==============================] - 16s 15ms/step - loss: 0.0994 - val_loss: 0.1010\n",
      "Epoch 12/200\n",
      "1016/1016 [==============================] - 16s 15ms/step - loss: 0.0958 - val_loss: 0.0922\n",
      "Epoch 13/200\n",
      "1016/1016 [==============================] - 16s 15ms/step - loss: 0.0929 - val_loss: 0.0894\n",
      "Epoch 14/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0907 - val_loss: 0.0884\n",
      "Epoch 15/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0914 - val_loss: 0.0867\n",
      "Epoch 16/200\n",
      "1016/1016 [==============================] - 16s 15ms/step - loss: 0.0883 - val_loss: 0.0849\n",
      "Epoch 17/200\n",
      "1016/1016 [==============================] - 16s 15ms/step - loss: 0.0863 - val_loss: 0.0845\n",
      "Epoch 18/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0855 - val_loss: 0.0817\n",
      "Epoch 19/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0831 - val_loss: 0.0801\n",
      "Epoch 20/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0820 - val_loss: 0.0816\n",
      "Epoch 21/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0812 - val_loss: 0.0784\n",
      "Epoch 22/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0803 - val_loss: 0.0781\n",
      "Epoch 23/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0797 - val_loss: 0.0774\n",
      "Epoch 24/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0781 - val_loss: 0.0757\n",
      "Epoch 25/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0780 - val_loss: 0.0760\n",
      "Epoch 26/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0768 - val_loss: 0.0742\n",
      "Epoch 27/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0762 - val_loss: 0.0771\n",
      "Epoch 28/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0765 - val_loss: 0.0735\n",
      "Epoch 29/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0757 - val_loss: 0.0733\n",
      "Epoch 30/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0759 - val_loss: 0.0724\n",
      "Epoch 31/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0741 - val_loss: 0.0737\n",
      "Epoch 32/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0734 - val_loss: 0.0724\n",
      "Epoch 33/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0732 - val_loss: 0.0719\n",
      "Epoch 34/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0730 - val_loss: 0.0715\n",
      "Epoch 35/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0723 - val_loss: 0.0703\n",
      "Epoch 36/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0727 - val_loss: 0.0745\n",
      "Epoch 37/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0722 - val_loss: 0.0701\n",
      "Epoch 38/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0712 - val_loss: 0.0704\n",
      "Epoch 39/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0712 - val_loss: 0.0688\n",
      "Epoch 40/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0709 - val_loss: 0.0694\n",
      "Epoch 41/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0710 - val_loss: 0.0688\n",
      "Epoch 42/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0702 - val_loss: 0.0686\n",
      "Epoch 43/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0694 - val_loss: 0.0674\n",
      "Epoch 44/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0693 - val_loss: 0.0800\n",
      "Epoch 45/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0705 - val_loss: 0.0674\n",
      "Epoch 46/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0689 - val_loss: 0.0671\n",
      "Epoch 47/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0684 - val_loss: 0.0666\n",
      "Epoch 48/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0686 - val_loss: 0.0665\n",
      "Epoch 49/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0683 - val_loss: 0.0669\n",
      "Epoch 50/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0677 - val_loss: 0.0669\n",
      "Epoch 51/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0677 - val_loss: 0.0662\n",
      "Epoch 52/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0676 - val_loss: 0.0663\n",
      "Epoch 53/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0671 - val_loss: 0.0657\n",
      "Epoch 54/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0671 - val_loss: 0.0657\n",
      "Epoch 55/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0664 - val_loss: 0.0658\n",
      "Epoch 56/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0666 - val_loss: 0.0653\n",
      "Epoch 57/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0671 - val_loss: 0.0654\n",
      "Epoch 58/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0669 - val_loss: 0.0646\n",
      "Epoch 59/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0662 - val_loss: 0.0663\n",
      "Epoch 60/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0655 - val_loss: 0.0646\n",
      "Epoch 61/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0658 - val_loss: 0.0653\n",
      "Epoch 62/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0650 - val_loss: 0.0643\n",
      "Epoch 63/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0652 - val_loss: 0.0638\n",
      "Epoch 64/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0654 - val_loss: 0.0643\n",
      "Epoch 65/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0659 - val_loss: 0.0639\n",
      "Epoch 66/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0649 - val_loss: 0.0646\n",
      "Epoch 67/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0650 - val_loss: 0.0656\n",
      "Epoch 68/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0644 - val_loss: 0.0636\n",
      "Epoch 69/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0657 - val_loss: 0.0632\n",
      "Epoch 70/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0645 - val_loss: 0.0670\n",
      "Epoch 71/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0645 - val_loss: 0.0661\n",
      "Epoch 72/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0645 - val_loss: 0.0634\n",
      "Epoch 73/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0652 - val_loss: 0.0638\n",
      "Epoch 74/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0641 - val_loss: 0.0630\n",
      "Epoch 75/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0638 - val_loss: 0.0633\n",
      "Epoch 76/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0651 - val_loss: 0.0675\n",
      "Epoch 77/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0688 - val_loss: 0.0688\n",
      "Epoch 78/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0652 - val_loss: 0.0634\n",
      "Epoch 79/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0642 - val_loss: 0.0629\n",
      "Epoch 80/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0638 - val_loss: 0.0623\n",
      "Epoch 81/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0634 - val_loss: 0.0626\n",
      "Epoch 82/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0643 - val_loss: 0.0621\n",
      "Epoch 83/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0629 - val_loss: 0.0642\n",
      "Epoch 84/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0631 - val_loss: 0.0626\n",
      "Epoch 85/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0626 - val_loss: 0.0613\n",
      "Epoch 86/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0635 - val_loss: 0.0617\n",
      "Epoch 87/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0639 - val_loss: 0.0626\n",
      "Epoch 88/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0632 - val_loss: 0.0615\n",
      "Epoch 89/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0623 - val_loss: 0.0636\n",
      "Epoch 90/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0628 - val_loss: 0.0619\n",
      "Epoch 91/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0633 - val_loss: 0.0689\n",
      "Epoch 92/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0624 - val_loss: 0.0607\n",
      "Epoch 93/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0622 - val_loss: 0.0620\n",
      "Epoch 94/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0629 - val_loss: 0.0612\n",
      "Epoch 95/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0620 - val_loss: 0.0608\n",
      "Epoch 96/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0630 - val_loss: 0.0697\n",
      "Epoch 97/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0622 - val_loss: 0.0604\n",
      "Epoch 98/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0619 - val_loss: 0.0610\n",
      "Epoch 99/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0615 - val_loss: 0.0604\n",
      "Epoch 100/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0619 - val_loss: 0.0610\n",
      "Epoch 101/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0616 - val_loss: 0.0617\n",
      "Epoch 102/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0617 - val_loss: 0.0598\n",
      "Epoch 103/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0611 - val_loss: 0.0609\n",
      "Epoch 104/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0612 - val_loss: 0.0610\n",
      "Epoch 105/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0617 - val_loss: 0.0608\n",
      "Epoch 106/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0609 - val_loss: 0.0597\n",
      "Epoch 107/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0611 - val_loss: 0.0594\n",
      "Epoch 108/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0613 - val_loss: 0.0603\n",
      "Epoch 109/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0613 - val_loss: 0.0619\n",
      "Epoch 110/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0614 - val_loss: 0.0598\n",
      "Epoch 111/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0608 - val_loss: 0.0626\n",
      "Epoch 112/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0612 - val_loss: 0.0603\n",
      "Epoch 113/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0611 - val_loss: 0.0601\n",
      "Epoch 114/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0605 - val_loss: 0.0598\n",
      "Epoch 115/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0609 - val_loss: 0.1152\n",
      "Epoch 116/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0658 - val_loss: 0.0605\n",
      "Epoch 117/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0608 - val_loss: 0.0596\n",
      "Epoch 118/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0603 - val_loss: 0.0589\n",
      "Epoch 119/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0601 - val_loss: 0.0612\n",
      "Epoch 120/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0604 - val_loss: 0.0611\n",
      "Epoch 121/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0613 - val_loss: 0.0600\n",
      "Epoch 122/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0616 - val_loss: 0.0595\n",
      "Epoch 123/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0599 - val_loss: 0.0601\n",
      "Epoch 124/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0598 - val_loss: 0.0588\n",
      "Epoch 125/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0606 - val_loss: 0.0598\n",
      "Epoch 126/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0604 - val_loss: 0.0587\n",
      "Epoch 127/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0598 - val_loss: 0.0616\n",
      "Epoch 128/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0603 - val_loss: 0.0598\n",
      "Epoch 129/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0597 - val_loss: 0.0592\n",
      "Epoch 130/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0597 - val_loss: 0.0614\n",
      "Epoch 131/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0596 - val_loss: 0.0584\n",
      "Epoch 132/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0604 - val_loss: 0.0582\n",
      "Epoch 133/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0594 - val_loss: 0.0593\n",
      "Epoch 134/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0601 - val_loss: 0.0589\n",
      "Epoch 135/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0600 - val_loss: 0.0597\n",
      "Epoch 136/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0595 - val_loss: 0.0592\n",
      "Epoch 137/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0595 - val_loss: 0.0587\n",
      "Epoch 138/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0599 - val_loss: 0.0587\n",
      "Epoch 139/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0599 - val_loss: 0.0605\n",
      "Epoch 140/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0601 - val_loss: 0.0596\n",
      "Epoch 141/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0599 - val_loss: 0.0607\n",
      "Epoch 142/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0595 - val_loss: 0.0591\n",
      "Epoch 143/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0597 - val_loss: 0.0580\n",
      "Epoch 144/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0596 - val_loss: 0.0584\n",
      "Epoch 145/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0589 - val_loss: 0.0585\n",
      "Epoch 146/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0589 - val_loss: 0.0579\n",
      "Epoch 147/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0594 - val_loss: 0.0597\n",
      "Epoch 148/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0607 - val_loss: 0.0634\n",
      "Epoch 149/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0611 - val_loss: 0.0584\n",
      "Epoch 150/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0586 - val_loss: 0.0579\n",
      "Epoch 151/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0588 - val_loss: 0.0607\n",
      "Epoch 152/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0591 - val_loss: 0.0581\n",
      "Epoch 153/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0592 - val_loss: 0.0593\n",
      "Epoch 154/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0590 - val_loss: 0.0624\n",
      "Epoch 155/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0602 - val_loss: 0.0583\n",
      "Epoch 156/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0589 - val_loss: 0.0587\n",
      "Epoch 157/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0604 - val_loss: 0.0587\n",
      "Epoch 158/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0588 - val_loss: 0.0603\n",
      "Epoch 159/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0588 - val_loss: 0.0584\n",
      "Epoch 160/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0589 - val_loss: 0.0582\n",
      "Epoch 161/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0589 - val_loss: 0.0581\n",
      "Epoch 162/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0594 - val_loss: 0.0591\n",
      "Epoch 163/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0582 - val_loss: 0.0574\n",
      "Epoch 164/200\n",
      "1016/1016 [==============================] - 18s 18ms/step - loss: 0.0589 - val_loss: 0.0578\n",
      "Epoch 165/200\n",
      "1016/1016 [==============================] - 17s 16ms/step - loss: 0.0592 - val_loss: 0.0573\n",
      "Epoch 166/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0584 - val_loss: 0.0577\n",
      "Epoch 167/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0580 - val_loss: 0.0570\n",
      "Epoch 168/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0578 - val_loss: 0.0574\n",
      "Epoch 169/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0584 - val_loss: 0.0611\n",
      "Epoch 170/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0607 - val_loss: 0.0575\n",
      "Epoch 171/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0582 - val_loss: 0.0570\n",
      "Epoch 172/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0587 - val_loss: 0.0593\n",
      "Epoch 173/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0597 - val_loss: 0.0581\n",
      "Epoch 174/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0581 - val_loss: 0.0593\n",
      "Epoch 175/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0578 - val_loss: 0.0579\n",
      "Epoch 176/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0579 - val_loss: 0.0570\n",
      "Epoch 177/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0576 - val_loss: 0.0578\n",
      "Epoch 178/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0580 - val_loss: 0.0570\n",
      "Epoch 179/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0580 - val_loss: 0.0602\n",
      "Epoch 180/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0591 - val_loss: 0.0580\n",
      "Epoch 181/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0589 - val_loss: 0.0574\n",
      "Epoch 182/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0578 - val_loss: 0.0622\n",
      "Epoch 183/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0645 - val_loss: 0.0636\n",
      "Epoch 184/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0608 - val_loss: 0.0602\n",
      "Epoch 185/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0588 - val_loss: 0.0593\n",
      "Epoch 186/200\n",
      "1016/1016 [==============================] - 17s 17ms/step - loss: 0.0586 - val_loss: 0.0597\n",
      "Epoch 187/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0583 - val_loss: 0.0568\n",
      "Epoch 188/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0578 - val_loss: 0.0573\n",
      "Epoch 189/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0592 - val_loss: 0.0597\n",
      "Epoch 190/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0586 - val_loss: 0.0615\n",
      "Epoch 191/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0581 - val_loss: 0.0566\n",
      "Epoch 192/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0570 - val_loss: 0.0568\n",
      "Epoch 193/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0575 - val_loss: 0.0569\n",
      "Epoch 194/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0587 - val_loss: 0.0566\n",
      "Epoch 195/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0574 - val_loss: 0.0566\n",
      "Epoch 196/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0581 - val_loss: 0.0573\n",
      "Epoch 197/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0577 - val_loss: 0.0571\n",
      "Epoch 198/200\n",
      "1016/1016 [==============================] - 18s 17ms/step - loss: 0.0575 - val_loss: 0.0561\n",
      "Epoch 199/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0593 - val_loss: 0.0575\n",
      "Epoch 200/200\n",
      "1016/1016 [==============================] - 16s 16ms/step - loss: 0.0574 - val_loss: 0.0569\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Predicted---\n",
      "[[[ 2.440e-01  6.380e-01  1.385e+00 ...  9.770e-01  2.600e-02 -1.600e-02]\n",
      "  [ 1.550e-01  4.700e-01  1.294e+00 ...  1.017e+00  2.000e-03 -5.000e-03]\n",
      "  [ 1.450e-01  3.930e-01  1.254e+00 ...  1.075e+00  9.600e-02 -7.600e-02]\n",
      "  [ 1.000e-01  3.280e-01  1.273e+00 ...  1.081e+00  8.100e-02 -1.180e-01]\n",
      "  [ 8.500e-02  2.740e-01  1.275e+00 ...  1.113e+00  1.220e-01 -1.710e-01]]\n",
      "\n",
      " [[ 2.500e-01 -7.390e-01  1.110e-01 ... -7.480e-01 -6.650e-01 -3.600e-01]\n",
      "  [ 1.860e-01 -7.780e-01  4.700e-02 ... -7.680e-01 -7.420e-01 -3.240e-01]\n",
      "  [ 1.740e-01 -7.960e-01  4.200e-02 ... -7.650e-01 -7.610e-01 -3.330e-01]\n",
      "  [ 1.720e-01 -7.850e-01  6.000e-02 ... -7.470e-01 -7.570e-01 -3.390e-01]\n",
      "  [ 1.740e-01 -7.770e-01  7.100e-02 ... -7.430e-01 -7.620e-01 -3.440e-01]]\n",
      "\n",
      " [[ 1.860e-01  1.549e+00  5.750e-01 ...  2.249e+00  3.145e+00 -5.600e-02]\n",
      "  [ 2.910e-01  1.623e+00  7.350e-01 ...  2.054e+00  3.247e+00  3.000e-02]\n",
      "  [ 2.720e-01  1.626e+00  7.550e-01 ...  2.071e+00  3.188e+00  1.800e-02]\n",
      "  [ 2.700e-01  1.611e+00  7.640e-01 ...  2.037e+00  3.222e+00  1.100e-02]\n",
      "  [ 2.770e-01  1.614e+00  7.660e-01 ...  2.022e+00  3.233e+00  8.000e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3.920e-01  2.280e-01  1.433e+00 ...  1.650e-01 -6.890e-01 -1.770e-01]\n",
      "  [ 2.930e-01  2.480e-01  1.411e+00 ...  1.610e-01 -5.760e-01 -2.030e-01]\n",
      "  [ 2.540e-01  2.370e-01  1.409e+00 ...  2.030e-01 -6.150e-01 -2.150e-01]\n",
      "  [ 2.470e-01  2.230e-01  1.426e+00 ...  2.310e-01 -5.980e-01 -2.210e-01]\n",
      "  [ 2.410e-01  2.100e-01  1.440e+00 ...  2.560e-01 -5.730e-01 -2.330e-01]]\n",
      "\n",
      " [[ 8.600e-02 -1.515e+00 -4.310e-01 ...  9.630e-01  8.260e-01 -9.800e-02]\n",
      "  [ 2.890e-01 -1.303e+00 -3.180e-01 ...  1.096e+00  1.146e+00 -8.300e-02]\n",
      "  [ 2.660e-01 -1.448e+00 -5.140e-01 ...  8.760e-01  1.053e+00 -7.100e-02]\n",
      "  [ 1.890e-01 -1.415e+00 -5.120e-01 ...  9.760e-01  9.180e-01 -1.190e-01]\n",
      "  [ 2.400e-01 -1.425e+00 -5.500e-01 ...  9.220e-01  9.630e-01 -1.140e-01]]\n",
      "\n",
      " [[ 1.920e-01  1.930e-01 -4.720e-01 ...  1.523e+00  5.870e-01 -1.420e-01]\n",
      "  [ 2.040e-01  1.860e-01 -5.570e-01 ...  1.714e+00  6.320e-01 -1.350e-01]\n",
      "  [ 2.440e-01  2.380e-01 -5.470e-01 ...  1.717e+00  6.090e-01 -1.170e-01]\n",
      "  [ 2.410e-01  2.610e-01 -5.370e-01 ...  1.725e+00  6.150e-01 -1.060e-01]\n",
      "  [ 2.470e-01  2.680e-01 -5.300e-01 ...  1.709e+00  6.240e-01 -1.000e-01]]]\n",
      "---Actual---\n",
      "[[[ 0.121  0.23   0.698 ...  1.772 -0.184 -0.16 ]\n",
      "  [ 0.166  0.23   0.721 ...  1.765 -0.119 -0.158]\n",
      "  [ 0.121  0.23   0.721 ...  1.765 -0.087 -0.138]\n",
      "  [ 0.15   0.269  0.698 ...  1.772 -0.055 -0.107]\n",
      "  [ 0.13   0.23   0.721 ...  1.743 -0.055 -0.083]]\n",
      "\n",
      " [[ 0.13  -0.778 -0.18  ... -0.512 -0.698 -0.202]\n",
      "  [ 0.15  -0.797 -0.203 ... -0.526 -0.698 -0.197]\n",
      "  [ 0.138 -0.758 -0.203 ... -0.523 -0.698 -0.196]\n",
      "  [ 0.146 -0.778 -0.203 ... -0.512 -0.731 -0.203]\n",
      "  [ 0.138 -0.758 -0.18  ... -0.482 -0.731 -0.213]]\n",
      "\n",
      " [[-0.057  1.741  1.159 ...  2.038  3.259 -0.013]\n",
      "  [-0.053  1.741  1.159 ...  2.126  3.276 -0.029]\n",
      "  [-0.081  1.741  1.159 ...  2.167  3.259 -0.052]\n",
      "  [-0.065  1.741  1.159 ...  2.115  3.211 -0.073]\n",
      "  [-0.065  1.741  1.183 ...  2.03   3.163 -0.087]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.117  0.288  1.183 ...  0.273 -0.409 -0.119]\n",
      "  [ 0.166  0.288  1.206 ...  0.247 -0.409 -0.115]\n",
      "  [ 0.117  0.288  1.183 ...  0.214 -0.409 -0.11 ]\n",
      "  [ 0.162  0.288  1.206 ...  0.214 -0.425 -0.103]\n",
      "  [ 0.13   0.288  1.183 ...  0.214 -0.409 -0.105]]\n",
      "\n",
      " [[ 0.146 -1.32  -0.341 ...  1.297  1.039  0.095]\n",
      "  [ 0.146 -1.32  -0.341 ...  1.389  1.023  0.095]\n",
      "  [ 0.146 -1.34  -0.064 ...  1.478  1.023  0.094]\n",
      "  [ 0.138 -1.32  -0.341 ...  1.518  1.039  0.088]\n",
      "  [ 0.138 -1.34  -0.341 ...  1.544  1.104  0.078]]\n",
      "\n",
      " [[ 0.138  0.366 -0.295 ...  1.562  0.444 -0.317]\n",
      "  [ 0.138  0.366 -0.249 ...  1.57   0.476 -0.319]\n",
      "  [ 0.138  0.366 -0.295 ...  1.574  0.46  -0.32 ]\n",
      "  [ 0.15   0.366 -0.295 ...  1.522  0.396 -0.325]\n",
      "  [ 0.13   0.366 -0.318 ...  1.463  0.363 -0.325]]]\n"
     ]
    }
   ],
   "source": [
    " # define model\n",
    "lstm_ae = keras.models.Sequential()\n",
    "# Encoder\n",
    "lstm_ae.add(keras.layers.LSTM(32, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "lstm_ae.add(keras.layers.LSTM(16, activation='relu', return_sequences=False))\n",
    "lstm_ae.add(keras.layers.RepeatVector(timesteps))\n",
    "# Decoder\n",
    "lstm_ae.add(keras.layers.LSTM(16, activation='relu', return_sequences=True))\n",
    "lstm_ae.add(keras.layers.LSTM(32, activation='relu', return_sequences=True))\n",
    "lstm_ae.add(keras.layers.TimeDistributed(keras.layers.Dense(n_features)))\n",
    "\n",
    "lstm_ae.summary()\n",
    "\n",
    "# compile\n",
    "lstm_ae.compile(loss='mse', optimizer=keras.optimizers.Adam(lr))\n",
    "\n",
    "# fit\n",
    "history = lstm_ae.fit(x_train_y1_scaled, x_train_y1_scaled,\n",
    "                     epochs=epochs, batch_size=batch,\n",
    "                     validation_data=(x_valid_y1_scaled, x_valid_y1_scaled))\n",
    "# plot result\n",
    "plt.plot(history.history['loss'], 'b', label='train loss')\n",
    "plt.plot(history.history['val_loss'], 'g', label='val loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# demonstrate reconstruction\n",
    "x_predict = lstm_ae.predict(x_test_scaled, verbose=0)\n",
    "print('---Predicted---')\n",
    "print(np.round(x_predict,3))\n",
    "print('---Actual---')\n",
    "print(np.round(x_test_scaled, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596cfa4",
   "metadata": {
    "id": "f596cfa4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b1a37",
   "metadata": {
    "id": "b49b1a37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3e4d4",
   "metadata": {
    "id": "ece3e4d4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082f36d7",
   "metadata": {
    "id": "082f36d7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a762afbb",
   "metadata": {
    "id": "a762afbb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
